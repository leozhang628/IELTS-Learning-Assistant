{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Gradio with writing evluating system+ tts + system workflow**"
      ],
      "metadata": {
        "id": "1fES_EktpboT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "Knz8xeekLPHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a68eac-2aee-46d5-cd33-3bc8eb6473f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.26.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.9.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.9.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio requests"
      ],
      "metadata": {
        "id": "vsLLcMIVOi2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ef0c2f-3fc0-4178-cdf0-a5a186b48d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.9.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.9.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xP7G32vTD70x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, VitsModel, AutoTokenizer as TTSTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Add language detection function\n",
        "def detect_language(text):\n",
        "    \"\"\"Detect whether the text is Chinese or English\"\"\"\n",
        "    # Simple judgment: if it contains Chinese characters, it is considered Chinese\n",
        "    for char in text:\n",
        "        if '\\u4e00' <= char <= '\\u9fff':\n",
        "            return \"zh\"\n",
        "    return \"en\"\n",
        "\n",
        "# 1. Load existing models and data\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\").to(device)\n",
        "\n",
        "# Simple user data storage\n",
        "user_progress = {}\n",
        "\n",
        "# Load your text data and embeddings\n",
        "try:\n",
        "    import pandas as pd\n",
        "    text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
        "    # Transform Embed\n",
        "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(\n",
        "        lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "    embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
        "                              dtype=torch.float32).to(device)\n",
        "    print(\"✅ Successfully loaded data\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load data: {e}\")\n",
        "    # If the data loading fails, you can provide some sample data\n",
        "    pages_and_chunks = []\n",
        "    embeddings = None\n",
        "\n",
        "# 2. Load LLM\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Add TTS model (lazy loading to save memory)\n",
        "tts_model = None\n",
        "tts_tokenizer = None\n",
        "\n",
        "def load_tts_model():\n",
        "    global tts_model, tts_tokenizer\n",
        "    if tts_model is None:\n",
        "        print(\"Loading TTS model...\")\n",
        "        tts_model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n",
        "        tts_tokenizer = TTSTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\n",
        "        print(\"✅ TTS model loaded\")\n",
        "    return tts_model, tts_tokenizer\n",
        "\n",
        "# 3. Define retrieval and generation functions\n",
        "def retrieve_relevant_resources(query, embeddings, n_resources=5):\n",
        "    \"\"\"Search for related resources\"\"\"\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "    scores, indices = torch.topk(dot_scores, k=n_resources)\n",
        "    return scores, indices\n",
        "\n",
        "def generate_answer(query, context_items, avg_relevance_score=0.0):\n",
        "    \"\"\"Generate answers, determine the answer language based on the query language, and decide whether to use the search content based on relevance\"\"\"\n",
        "    # Set a relevance threshold below which LLM knowledge is used instead of search content\n",
        "    relevance_threshold = 0.65\n",
        "\n",
        "    # All output uses English by default\n",
        "    language = \"en\"\n",
        "\n",
        "    # Determine the prompt content based on relevance\n",
        "    if avg_relevance_score < relevance_threshold:\n",
        "        # Low relevance, let the model use its own knowledge\n",
        "        prompt = f\"\"\"Here is a question about the IELTS exam. Since no sufficiently relevant reference materials were found, please use your own knowledge to answer.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    else:\n",
        "        # Highly relevant, using the retrieved content\n",
        "        context = \"\\n\\n\".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "\n",
        "        prompt = f\"\"\"Based on the following IELTS materials, answer the question:\n",
        "\n",
        "Content:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\")\n",
        "    return response, language\n",
        "\n",
        "def evaluate_response_quality(query, response, relevance_score):\n",
        "    \"\"\"Evaluate the quality of the system's answers\"\"\"\n",
        "    # Evaluate based on relevance and heuristic rules\n",
        "    coherence_score = min(1.0, 0.5 + relevance_score * 0.5)  # Simple heuristic rules\n",
        "\n",
        "    # Generate English evaluation report\n",
        "    if relevance_score > 0.8:\n",
        "        relevance_comment = \"Highly Relevant - The answer directly addresses the core of the question\"\n",
        "    elif relevance_score > 0.6:\n",
        "        relevance_comment = \"Relevant - The answer covers main aspects of the question\"\n",
        "    else:\n",
        "        relevance_comment = \"Partially Relevant - The answer only partially addresses the question\"\n",
        "\n",
        "    # Text length evaluation\n",
        "    if len(response) < 50:\n",
        "        length_comment = \"Too Short - The answer may not be comprehensive\"\n",
        "    elif len(response) > 500:\n",
        "        length_comment = \"Extensive - The answer is very comprehensive\"\n",
        "    else:\n",
        "        length_comment = \"Adequate - The answer length is reasonable\"\n",
        "\n",
        "    # Portfolio Assessment Report\n",
        "    report = f\"\"\"\n",
        "### Answer Quality Assessment\n",
        "\n",
        "**Relevance**: {relevance_score:.2f}/1.0 - {relevance_comment}\n",
        "**Coherence**: {coherence_score:.2f}/1.0\n",
        "**Length**: {len(response)} characters - {length_comment}\n",
        "\n",
        "**Overall Rating**: {(relevance_score + coherence_score) / 2:.2f}/1.0\n",
        "    \"\"\"\n",
        "\n",
        "    return report\n",
        "\n",
        "def process_query(query):\n",
        "    \"\"\"Processes the query and returns results and performance indicators (improved version with relevance evaluation)\"\"\"\n",
        "    # Check if data has been loaded\n",
        "    if embeddings is None:\n",
        "        return \"Data not loaded, please run the data processing code first\", \"\", \"Performance metrics unavailable: data not loaded\"\n",
        "\n",
        "    # Recording start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get relevant resources\n",
        "    retrieval_start = time.time()\n",
        "    scores, indices = retrieve_relevant_resources(query, embeddings)\n",
        "    retrieval_time = time.time() - retrieval_start\n",
        "\n",
        "    context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "    # Calculate the average relevance score\n",
        "    avg_relevance = scores.mean().item()\n",
        "\n",
        "    # Record generation start time\n",
        "    generation_start = time.time()\n",
        "\n",
        "    # Generate Answer (now passes relevance score)\n",
        "    answer, detected_language = generate_answer(query, context_items, avg_relevance)\n",
        "\n",
        "    # Calculate generation time\n",
        "    generation_time = time.time() - generation_start\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Preparing context information for display\n",
        "    context_display = \"\"\n",
        "    for i, (score, idx) in enumerate(zip(scores, indices)):\n",
        "        context_display += f\"**Reference {i+1}** (Relevance: {score:.2f}):\\n{pages_and_chunks[idx]['sentence_chunk'][:200]}...\\n\\n\"\n",
        "\n",
        "    # Quality Assessment\n",
        "    quality_report = evaluate_response_quality(query, answer, avg_relevance)\n",
        "\n",
        "    # Preparing performance indicators\n",
        "    metrics = f\"\"\"\n",
        "### System Performance Metrics\n",
        "- **Retrieval Time**: {retrieval_time:.2f} seconds\n",
        "- **Generation Time**: {generation_time:.2f} seconds\n",
        "- **Total Response Time**: {total_time:.2f} seconds\n",
        "- **Average Relevance Score**: {avg_relevance:.4f}/1.0\n",
        "- **Response Mode**: {\"Retrieved Context\" if avg_relevance >= 0.65 else \"LLM Knowledge\"}\n",
        "{quality_report}\n",
        "\"\"\"\n",
        "\n",
        "    return answer, context_display, metrics\n",
        "\n",
        "def process_query_with_history(query, history=\"\"):\n",
        "    \"\"\"Processing queries and saving history\"\"\"\n",
        "    answer, context, metrics = process_query(query)\n",
        "\n",
        "    # Update History\n",
        "    timestamp = time.strftime(\"%H:%M:%S\")\n",
        "    new_history = f\"{history}<hr><b>[{timestamp}] Q:</b> {query}<br><b>A:</b> {answer}<br>\"\n",
        "\n",
        "    return answer, context, metrics, new_history\n",
        "\n",
        "def track_user_activity(username, activity_type, content, score=None):\n",
        "    \"\"\"Tracking user learning activities\"\"\"\n",
        "    if username not in user_progress:\n",
        "        user_progress[username] = {\n",
        "            \"queries\": [],\n",
        "            \"writing_samples\": [],\n",
        "            \"practice_tests\": [],\n",
        "            \"last_active\": None\n",
        "        }\n",
        "\n",
        "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    if activity_type == \"query\":\n",
        "        user_progress[username][\"queries\"].append({\n",
        "            \"timestamp\": timestamp,\n",
        "            \"query\": content,\n",
        "            \"relevance_score\": score\n",
        "        })\n",
        "    elif activity_type == \"writing\":\n",
        "        user_progress[username][\"writing_samples\"].append({\n",
        "            \"timestamp\": timestamp,\n",
        "            \"sample\": content[:100] + \"...\",  # Save Summary\n",
        "            \"score\": score\n",
        "        })\n",
        "    elif activity_type == \"practice\":\n",
        "        user_progress[username][\"practice_tests\"].append({\n",
        "            \"timestamp\": timestamp,\n",
        "            \"test_type\": content,\n",
        "            \"completed\": True\n",
        "        })\n",
        "\n",
        "    user_progress[username][\"last_active\"] = timestamp\n",
        "\n",
        "    # Build Progress Summary\n",
        "    summary = f\"\"\"\n",
        "### Learning Progress Summary ({username})\n",
        "- Questions asked: {len(user_progress[username][\"queries\"])}\n",
        "- Writing samples: {len(user_progress[username][\"writing_samples\"])}\n",
        "- Practice tests: {len(user_progress[username][\"practice_tests\"])}\n",
        "- Last active: {user_progress[username][\"last_active\"]}\n",
        "\n",
        "#### Recent Activity\n",
        "\"\"\"\n",
        "\n",
        "    # Add the last 5 events\n",
        "    recent_queries = user_progress[username][\"queries\"][-3:] if user_progress[username][\"queries\"] else []\n",
        "    recent_writings = user_progress[username][\"writing_samples\"][-2:] if user_progress[username][\"writing_samples\"] else []\n",
        "\n",
        "    for q in recent_queries:\n",
        "        summary += f\"- [{q['timestamp']}] Question: {q['query'][:50]}...\\n\"\n",
        "\n",
        "    for w in recent_writings:\n",
        "        summary += f\"- [{w['timestamp']}] Writing practice\\n\"\n",
        "\n",
        "    return summary\n",
        "\n",
        "def process_query_with_tracking(query, username):\n",
        "    \"\"\"Handle inquiries and track learning progress\"\"\"\n",
        "    answer, context, metrics = process_query(query)\n",
        "\n",
        "    # Extracting relevance scores from metrics\n",
        "    import re\n",
        "    relevance_match = re.search(r\"Average Relevance Score: ([\\d\\.]+)\", metrics)\n",
        "    relevance_score = float(relevance_match.group(1)) if relevance_match else None\n",
        "\n",
        "    # Track this query\n",
        "    progress = track_user_activity(username, \"query\", query, relevance_score)\n",
        "\n",
        "    return answer, context, metrics, progress\n",
        "\n",
        "# Modify the TTS function to ensure that the complete content is processed\n",
        "def text_to_speech(text):\n",
        "    \"\"\"Convert text to speech, process full English text\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    # Load the TTS model (if not already loaded)\n",
        "    model, tokenizer = load_tts_model()\n",
        "\n",
        "    # If the text is too long, process it in segments and concatenate them\n",
        "    max_segment_length = 500  # Maximum length of each segment\n",
        "    segments = []\n",
        "\n",
        "    # Segment text\n",
        "    if len(text) > max_segment_length:\n",
        "        words = text.split()\n",
        "        current_segment = []\n",
        "        current_length = 0\n",
        "\n",
        "        for word in words:\n",
        "            current_length += len(word) + 1  # +1 for space\n",
        "            if current_length <= max_segment_length:\n",
        "                current_segment.append(word)\n",
        "            else:\n",
        "                segments.append(\" \".join(current_segment))\n",
        "                current_segment = [word]\n",
        "                current_length = len(word) + 1\n",
        "\n",
        "        if current_segment:\n",
        "            segments.append(\" \".join(current_segment))\n",
        "    else:\n",
        "        segments = [text]\n",
        "\n",
        "    # Process each paragraph and concatenate\n",
        "    full_waveform = None\n",
        "    sample_rate = None\n",
        "\n",
        "    for segment in segments:\n",
        "        inputs = tokenizer(segment, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            output = model(**inputs).waveform\n",
        "\n",
        "        if full_waveform is None:\n",
        "            full_waveform = output[0].numpy()\n",
        "            sample_rate = model.config.sampling_rate\n",
        "        else:\n",
        "            # Add a short pause (0.3 seconds of silence)\n",
        "            pause = np.zeros(int(0.3 * sample_rate))\n",
        "            full_waveform = np.concatenate([full_waveform, pause, output[0].numpy()])\n",
        "\n",
        "    # Return to full audio\n",
        "    return (sample_rate, full_waveform)\n",
        "\n",
        "# Text-to-speech language detection wrapper function\n",
        "def tts_with_language_check(text):\n",
        "    if not text:\n",
        "        return None, \"Please provide text content\"\n",
        "\n",
        "    language = detect_language(text)\n",
        "    if language == \"zh\":\n",
        "        return None, \"⚠️ Only English text is supported for TTS. Please provide English text.\"\n",
        "    else:\n",
        "        try:\n",
        "            audio = text_to_speech(text)\n",
        "            return audio, \"✅ Conversion successful! Full content converted to speech.\"\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Error during conversion: {str(e)}\"\n",
        "\n",
        "# Added IELTS writing scoring function\n",
        "def evaluate_ielts_writing(writing_sample, username=\"default_user\"):\n",
        "    \"\"\"Evaluate IELTS writing samples and keep track of records\"\"\"\n",
        "    if not writing_sample:\n",
        "        return \"Please provide a writing sample for evaluation.\"\n",
        "\n",
        "    prompt = f\"\"\"As an IELTS examiner, please assess the following student writing sample.\n",
        "    Provide scores and specific suggestions based on these criteria:\n",
        "    1. Task Response\n",
        "    2. Coherence and Cohesion\n",
        "    3. Lexical Resource\n",
        "    4. Grammatical Range and Accuracy\n",
        "\n",
        "    Give scores in 0.5 increments (e.g., 6.0, 6.5) for each category, and provide an overall score.\n",
        "\n",
        "    Student writing sample:\n",
        "    {writing_sample}\n",
        "\n",
        "    Score and detailed feedback:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.2,\n",
        "            do_sample=True\n",
        "        )\n",
        "    feedback = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\")\n",
        "\n",
        "    # Extract total score\n",
        "    import re\n",
        "    score_match = re.search(r\"Overall score.*?(\\d+\\.?\\d*)\", feedback)\n",
        "    overall_score = float(score_match.group(1)) if score_match else None\n",
        "\n",
        "    # Record writing activities\n",
        "    track_user_activity(username, \"writing\", writing_sample, overall_score)\n",
        "\n",
        "    return feedback\n",
        "\n",
        "# Added mock exam feature\n",
        "def generate_practice_question(section_type):\n",
        "    \"\"\"Generate IELTS practice questions\"\"\"\n",
        "    section_type_english = section_type.split(\" \")[0]  # Get the English section\n",
        "\n",
        "    prompt = f\"\"\"Create an IELTS {section_type_english} practice question.\n",
        "    Include detailed questions, guidance, and scoring criteria.\n",
        "    For Writing or Speaking sections, provide a sample question and response framework.\n",
        "    For Listening or Reading sections, provide sample questions and answer options.\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "    practice = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\")\n",
        "    return practice\n",
        "\n",
        "# Add learning plan generation function\n",
        "def generate_study_plan(target_score, weeks_available, strengths, weaknesses):\n",
        "    \"\"\"Generate a personalized IELTS study plan\"\"\"\n",
        "    prompt = f\"\"\"Create a personalized IELTS study plan with the following conditions:\n",
        "\n",
        "    Target Score: {target_score}\n",
        "    Available Time: {weeks_available} weeks\n",
        "    Strengths: {strengths}\n",
        "    Weaknesses: {weaknesses}\n",
        "\n",
        "    Please provide:\n",
        "    1. Detailed weekly study plan\n",
        "    2. Recommended learning resources\n",
        "    3. Specific exercises for weaknesses\n",
        "    4. Regular mock test schedule\n",
        "    5. Pre-exam preparation strategy\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "    plan = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\")\n",
        "    return plan\n",
        "\n",
        "def export_history(history):\n",
        "    \"\"\"Export session history as text\"\"\"\n",
        "    if not history:\n",
        "        return \"No conversation history to export\"\n",
        "\n",
        "    try:\n",
        "        from bs4 import BeautifulSoup\n",
        "        import re\n",
        "\n",
        "        # Parsing HTML and extracting text using BeautifulSoup\n",
        "        soup = BeautifulSoup(history, \"html.parser\")\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Cleaning up the text\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Returns the cleaned text and timestamp\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        return f\"Conversation exported. Filename: ielts_conversation_{timestamp}.txt\\n\\n{cleaned_text[:100]}...\"\n",
        "    except:\n",
        "        # Simple alternate extraction\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        return f\"Conversation exported. Filename: ielts_conversation_{timestamp}.txt\"\n",
        "\n",
        "# Improve system architecture diagram generation function\n",
        "def generate_system_diagram():\n",
        "    \"\"\"Generate comprehensive system architecture diagram\"\"\"\n",
        "    from PIL import Image, ImageDraw, ImageFont\n",
        "    import io\n",
        "    import base64\n",
        "    import os\n",
        "\n",
        "    # Create a larger image to show the full content\n",
        "    width, height = 1000, 650\n",
        "    image = Image.new(\"RGB\", (width, height), \"white\")\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Try loading a better font\n",
        "    try:\n",
        "        # Try common fonts, depending on the system\n",
        "        font_paths = [\n",
        "            \"arial.ttf\",\n",
        "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
        "            \"/System/Library/Fonts/Helvetica.ttc\"\n",
        "        ]\n",
        "\n",
        "        font = None\n",
        "        for path in font_paths:\n",
        "            if os.path.exists(path):\n",
        "                font = ImageFont.truetype(path, 16)\n",
        "                title_font = ImageFont.truetype(path, 24)\n",
        "                break\n",
        "\n",
        "        if font is None:\n",
        "            font = ImageFont.load_default()\n",
        "            title_font = font\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "        title_font = font\n",
        "\n",
        "    # Define the main process box\n",
        "    main_flow_boxes = [\n",
        "        {\"x\": 100, \"y\": 150, \"width\": 150, \"height\": 80, \"text\": \"User Query\", \"color\": \"#FFD580\", \"description\": \"Questions about IELTS\"},\n",
        "        {\"x\": 320, \"y\": 150, \"width\": 150, \"height\": 80, \"text\": \"Vector Retrieval\", \"color\": \"#90EE90\", \"description\": \"Embedding search\"},\n",
        "        {\"x\": 540, \"y\": 150, \"width\": 150, \"height\": 80, \"text\": \"Relevant Content\", \"color\": \"#ADD8E6\", \"description\": \"Retrieved materials\"},\n",
        "        {\"x\": 760, \"y\": 150, \"width\": 150, \"height\": 80, \"text\": \"LLM Generation\", \"color\": \"#DDA0DD\", \"description\": \"Gemma-2b-it model\"},\n",
        "        {\"x\": 760, \"y\": 300, \"width\": 150, \"height\": 80, \"text\": \"Result Display\", \"color\": \"#FFCCCB\", \"description\": \"Answer with context\"}\n",
        "    ]\n",
        "\n",
        "    # Define other functional modules\n",
        "    feature_boxes = [\n",
        "        {\"x\": 100, \"y\": 300, \"width\": 150, \"height\": 80, \"text\": \"Writing Assessment\", \"color\": \"#F0E68C\", \"description\": \"Essay evaluation\"},\n",
        "        {\"x\": 320, \"y\": 300, \"width\": 150, \"height\": 80, \"text\": \"Study Plan\", \"color\": \"#98FB98\", \"description\": \"Learning roadmap\"},\n",
        "        {\"x\": 540, \"y\": 300, \"width\": 150, \"height\": 80, \"text\": \"Practice Tests\", \"color\": \"#87CEEB\", \"description\": \"Mock exams\"},\n",
        "        {\"x\": 320, \"y\": 450, \"width\": 150, \"height\": 80, \"text\": \"Text-to-Speech\", \"color\": \"#D8BFD8\", \"description\": \"Voice output\"},\n",
        "        {\"x\": 540, \"y\": 450, \"width\": 150, \"height\": 80, \"text\": \"Progress Tracking\", \"color\": \"#FFB6C1\", \"description\": \"Learning journey\"}\n",
        "    ]\n",
        "\n",
        "    # Draw the main title\n",
        "    title = \"IELTS Learning Assistant System Architecture\"\n",
        "    title_w, title_h = draw.textsize(title, font=title_font) if hasattr(draw, 'textsize') else (width//2, 40)\n",
        "    draw.text(((width - title_w) // 2, 30), title, fill=\"#4B0082\", font=title_font)\n",
        "\n",
        "    # Draw the main process box and description\n",
        "    for box in main_flow_boxes:\n",
        "        x, y = box[\"x\"], box[\"y\"]\n",
        "        w, h = box[\"width\"], box[\"height\"]\n",
        "        draw.rectangle([(x, y), (x+w, y+h)], fill=box[\"color\"], outline=\"#000000\", width=2)\n",
        "\n",
        "        # Adding Main Text\n",
        "        text_w, text_h = draw.textsize(box[\"text\"], font=font) if hasattr(draw, 'textsize') else (w//2, h//3)\n",
        "        text_x = x + (w - text_w) // 2\n",
        "        text_y = y + (h - text_h) // 3\n",
        "        draw.text((text_x, text_y), box[\"text\"], fill=\"#000000\", font=font)\n",
        "\n",
        "        # Add description text\n",
        "        desc_w, desc_h = draw.textsize(box[\"description\"], font=font) if hasattr(draw, 'textsize') else (w//2, h//3)\n",
        "        desc_x = x + (w - desc_w) // 2\n",
        "        desc_y = y + h - text_h - 10\n",
        "        draw.text((desc_x, desc_y), box[\"description\"], fill=\"#333333\", font=font)\n",
        "\n",
        "    # Draw function module boxes and descriptions\n",
        "    for box in feature_boxes:\n",
        "        x, y = box[\"x\"], box[\"y\"]\n",
        "        w, h = box[\"width\"], box[\"height\"]\n",
        "        draw.rectangle([(x, y), (x+w, y+h)], fill=box[\"color\"], outline=\"#000000\", width=2)\n",
        "\n",
        "        # Adding Main Text\n",
        "        text_w, text_h = draw.textsize(box[\"text\"], font=font) if hasattr(draw, 'textsize') else (w//2, h//3)\n",
        "        text_x = x + (w - text_w) // 2\n",
        "        text_y = y + (h - text_h) // 3\n",
        "        draw.text((text_x, text_y), box[\"text\"], fill=\"#000000\", font=font)\n",
        "\n",
        "        # Add description text\n",
        "        desc_w, desc_h = draw.textsize(box[\"description\"], font=font) if hasattr(draw, 'textsize') else (w//2, h//3)\n",
        "        desc_x = x + (w - desc_w) // 2\n",
        "        desc_y = y + h - text_h - 10\n",
        "        draw.text((desc_x, desc_y), box[\"description\"], fill=\"#333333\", font=font)\n",
        "\n",
        "    # Drawing Connection Lines - Main Process\n",
        "    for i in range(len(main_flow_boxes)-2):  # The last frame is processed separately\n",
        "        x1 = main_flow_boxes[i][\"x\"] + main_flow_boxes[i][\"width\"]\n",
        "        y1 = main_flow_boxes[i][\"y\"] + main_flow_boxes[i][\"height\"]//2\n",
        "        x2 = main_flow_boxes[i+1][\"x\"]\n",
        "        y2 = main_flow_boxes[i+1][\"y\"] + main_flow_boxes[i+1][\"height\"]//2\n",
        "\n",
        "        # Lire\n",
        "        draw.line([(x1, y1), (x2, y2)], fill=\"#000000\", width=3)\n",
        "\n",
        "        # Arrow\n",
        "        arrow_size = 10\n",
        "        draw.polygon([(x2-arrow_size, y2-arrow_size//2), (x2, y2), (x2-arrow_size, y2+arrow_size//2)], fill=\"#000000\")\n",
        "\n",
        "    # Connection from LLM Generation to Result Display\n",
        "    llm_idx = 3  # LLM Generation Index\n",
        "    result_idx = 4  #Result Display Index\n",
        "\n",
        "    x1 = main_flow_boxes[llm_idx][\"x\"] + main_flow_boxes[llm_idx][\"width\"]//2\n",
        "    y1 = main_flow_boxes[llm_idx][\"y\"] + main_flow_boxes[llm_idx][\"height\"]\n",
        "    x2 = main_flow_boxes[result_idx][\"x\"] + main_flow_boxes[result_idx][\"width\"]//2\n",
        "    y2 = main_flow_boxes[result_idx][\"y\"]\n",
        "\n",
        "    # Lines and arrows\n",
        "    draw.line([(x1, y1), (x1, y1+30), (x2, y1+30), (x2, y2)], fill=\"#000000\", width=3)\n",
        "    arrow_size = 10\n",
        "    draw.polygon([(x2-arrow_size//2, y2-arrow_size), (x2, y2), (x2+arrow_size//2, y2-arrow_size)], fill=\"#000000\")\n",
        "\n",
        "    # LLM Generation to TTS connection\n",
        "    tts_idx = 3  # Text-to-Speech Index\n",
        "\n",
        "    x1 = main_flow_boxes[llm_idx][\"x\"]\n",
        "    y1 = main_flow_boxes[llm_idx][\"y\"] + main_flow_boxes[llm_idx][\"height\"]//2\n",
        "    x2 = feature_boxes[tts_idx][\"x\"] + feature_boxes[tts_idx][\"width\"]//2\n",
        "    y2 = feature_boxes[tts_idx][\"y\"]\n",
        "\n",
        "    draw.line([(x1, y1), (x1-30, y1), (x1-30, y2-30), (x2, y2-30), (x2, y2)], fill=\"#000000\", width=2)\n",
        "    arrow_size = 10\n",
        "    draw.polygon([(x2-arrow_size//2, y2-arrow_size), (x2, y2), (x2+arrow_size//2, y2-arrow_size)], fill=\"#000000\")\n",
        "\n",
        "    #Add a legend\n",
        "    legend_y = 580\n",
        "    legend_items = [\n",
        "        {\"text\": \"Main Process Flow\", \"color\": \"#000000\", \"width\": 3},\n",
        "        {\"text\": \"Additional Features\", \"color\": \"#000000\", \"width\": 2},\n",
        "    ]\n",
        "\n",
        "    for i, item in enumerate(legend_items):\n",
        "        x_pos = 100 + i * 300\n",
        "        # Line\n",
        "        draw.line([(x_pos, legend_y), (x_pos + 50, legend_y)], fill=item[\"color\"], width=item[\"width\"])\n",
        "        # text\n",
        "        draw.text((x_pos + 60, legend_y - 5), item[\"text\"], fill=\"#000000\", font=font)\n",
        "\n",
        "    # Add bottom note\n",
        "    footer_text = \"Built with SentenceTransformer, Google Gemma-2b-it, and Facebook MMS-TTS-Eng\"\n",
        "    footer_w, footer_h = draw.textsize(footer_text, font=font) if hasattr(draw, 'textsize') else (width//2, 20)\n",
        "    draw.text(((width - footer_w) // 2, height - 30), footer_text, fill=\"#666666\", font=font)\n",
        "\n",
        "    return image\n",
        "\n",
        "# 4. Creating the Gradio Interface\n",
        "def create_interface():\n",
        "    \"\"\"Creating the Gradio Interface\"\"\"\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "        with gr.Tab(\"Main Application\"):\n",
        "            gr.Markdown(\"# IELTS Learning Assistant\")\n",
        "            gr.Markdown(\"\"\"\n",
        "            This application uses artificial intelligence to answer questions about the IELTS exam. It is based on IELTS textbook content and can help you understand key aspects of the exam and improve your skills.\n",
        "\n",
        "            **Language Support**:\n",
        "            - Text-to-speech functionality is only available for English\n",
        "            \"\"\")\n",
        "\n",
        "            # User Information\n",
        "            with gr.Row():\n",
        "                username_input = gr.Textbox(\n",
        "                    label=\"Username\",\n",
        "                    placeholder=\"Enter your username to track learning progress\",\n",
        "                    value=\"default_user\"\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    query_input = gr.Textbox(\n",
        "                        label=\"Question\",\n",
        "                        placeholder=\"Enter your question about the IELTS exam...\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                    with gr.Row():\n",
        "                        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "                        clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "                    response_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=10,\n",
        "                        placeholder=\"AI's answer will appear here...\",\n",
        "                    )\n",
        "\n",
        "                    # New: Independent performance indicator display\n",
        "                    metrics_output = gr.Markdown(label=\"Performance Metrics\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    with gr.Accordion(\"References\", open=False):\n",
        "                        context_output = gr.Markdown()\n",
        "\n",
        "            # Session History\n",
        "            with gr.Accordion(\"Conversation History\", open=False):\n",
        "                history_display = gr.HTML()\n",
        "                with gr.Row():\n",
        "                    export_btn = gr.Button(\"Export Conversation\")\n",
        "                    clear_history_btn = gr.Button(\"Clear History\")\n",
        "                export_status = gr.Textbox(label=\"Export Status\", visible=False)\n",
        "\n",
        "            # Example Question\n",
        "            examples = [\n",
        "                [\"How can I improve my IELTS listening score?\"],\n",
        "                [\"What is the ideal structure for IELTS Writing Task 2?\"],\n",
        "                [\"How to manage time effectively in the IELTS reading test?\"],\n",
        "                [\"What should I pay attention to in the IELTS speaking test?\"],\n",
        "                [\"What are the common mistakes in IELTS Writing Task 1?\"],\n",
        "                [\"How to achieve band 7+ in IELTS?\"]\n",
        "            ]\n",
        "\n",
        "            # Setup Events - Updated to use tracking and history\n",
        "            submit_btn.click(\n",
        "                fn=process_query_with_history,\n",
        "                inputs=[query_input, history_display],\n",
        "                outputs=[response_output, context_output, metrics_output, history_display]\n",
        "            )\n",
        "            clear_btn.click(\n",
        "                lambda: [\"\", \"\", \"\"],\n",
        "                outputs=[query_input, response_output, metrics_output]\n",
        "            )\n",
        "            export_btn.click(\n",
        "                fn=export_history,\n",
        "                inputs=history_display,\n",
        "                outputs=export_status\n",
        "            )\n",
        "            clear_history_btn.click(\n",
        "                lambda: \"\",\n",
        "                outputs=history_display\n",
        "            )\n",
        "            gr.Examples(\n",
        "                examples=examples,\n",
        "                inputs=query_input\n",
        "            )\n",
        "\n",
        "            # Adding text-to-speech functionality\n",
        "            with gr.Accordion(\"Text-to-Speech Feature\", open=False):\n",
        "                gr.Markdown(\"## Convert Text to Speech\")\n",
        "                gr.Markdown(\"Enter English text or use the answer content to convert to speech.\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    text_for_tts = gr.Textbox(\n",
        "                        label=\"Text to convert to speech\",\n",
        "                        lines=2,\n",
        "                        placeholder=\"Enter English text to convert to speech...\"\n",
        "                    )\n",
        "                    with gr.Column():\n",
        "                        convert_btn = gr.Button(\"Convert Text\", variant=\"secondary\")\n",
        "                        convert_response_btn = gr.Button(\"Convert Answer\", variant=\"secondary\")\n",
        "\n",
        "                audio_output = gr.Audio(label=\"Speech Output\")\n",
        "                tts_status = gr.Markdown()  # Add status information display\n",
        "\n",
        "                # Connecting TTS Function\n",
        "                convert_btn.click(\n",
        "                    fn=tts_with_language_check,\n",
        "                    inputs=text_for_tts,\n",
        "                    outputs=[audio_output, tts_status]\n",
        "                )\n",
        "                convert_response_btn.click(\n",
        "                    fn=tts_with_language_check,\n",
        "                    inputs=response_output,\n",
        "                    outputs=[audio_output, tts_status]\n",
        "                )\n",
        "\n",
        "            # Add learning progress display\n",
        "            view_progress_btn = gr.Button(\"View Learning Progress\")\n",
        "            progress_display = gr.Markdown(label=\"Learning Progress\")\n",
        "\n",
        "            view_progress_btn.click(\n",
        "                fn=lambda username: track_user_activity(username, \"query\", \"View progress\"),\n",
        "                inputs=username_input,\n",
        "                outputs=progress_display\n",
        "            )\n",
        "\n",
        "        # Writing Grading Tab\n",
        "        with gr.Tab(\"Writing Assessment\"):\n",
        "            gr.Markdown(\"# IELTS Writing Assessment\")\n",
        "            gr.Markdown(\"Upload your IELTS writing sample to get professional scoring and feedback.\")\n",
        "\n",
        "            writing_username = gr.Textbox(\n",
        "                label=\"Username\",\n",
        "                placeholder=\"Enter your username to track progress\",\n",
        "                value=\"default_user\"\n",
        "            )\n",
        "\n",
        "            writing_input = gr.Textbox(\n",
        "                label=\"Paste your IELTS writing sample\",\n",
        "                placeholder=\"Paste your Task 1 or Task 2 writing here...\",\n",
        "                lines=10\n",
        "            )\n",
        "            evaluate_btn = gr.Button(\"Get Assessment\", variant=\"primary\")\n",
        "            evaluation_output = gr.Markdown(label=\"Scoring and Feedback\")\n",
        "            writing_progress = gr.Markdown(label=\"Writing Progress\")\n",
        "\n",
        "            # Updated writing grades to track progress\n",
        "            def evaluate_with_tracking(sample, username):\n",
        "                feedback = evaluate_ielts_writing(sample, username)\n",
        "                progress = track_user_activity(username, \"writing\", sample)\n",
        "                return feedback, progress\n",
        "\n",
        "            evaluate_btn.click(\n",
        "                fn=evaluate_with_tracking,\n",
        "                inputs=[writing_input, writing_username],\n",
        "                outputs=[evaluation_output, writing_progress]\n",
        "            )\n",
        "\n",
        "        # Added mock exam tab\n",
        "        with gr.Tab(\"Practice Tests\"):\n",
        "            gr.Markdown(\"# IELTS Practice Tests\")\n",
        "            gr.Markdown(\"Select an exam section to generate corresponding practice questions.\")\n",
        "\n",
        "            practice_username = gr.Textbox(\n",
        "                label=\"Username\",\n",
        "                placeholder=\"Enter your username to track progress\",\n",
        "                value=\"default_user\"\n",
        "            )\n",
        "\n",
        "            section_selector = gr.Dropdown(\n",
        "                label=\"Select Exam Section\",\n",
        "                choices=[\"Listening\", \"Reading\", \"Writing\", \"Speaking\"],\n",
        "                value=\"Writing\"\n",
        "            )\n",
        "\n",
        "            generate_btn = gr.Button(\"Generate Practice Question\", variant=\"primary\")\n",
        "            practice_output = gr.Markdown(label=\"Practice Question\")\n",
        "\n",
        "            # Generate mock questions and track activity\n",
        "            def generate_practice_with_tracking(section, username):\n",
        "                practice = generate_practice_question(section)\n",
        "                # Record this mock exam activity\n",
        "                track_user_activity(username, \"practice\", section)\n",
        "                return practice\n",
        "\n",
        "            generate_btn.click(\n",
        "                fn=generate_practice_with_tracking,\n",
        "                inputs=[section_selector, practice_username],\n",
        "                outputs=practice_output\n",
        "            )\n",
        "\n",
        "        # Add a learning plan tab\n",
        "        with gr.Tab(\"Study Plan\"):\n",
        "            gr.Markdown(\"# Personalized IELTS Study Plan\")\n",
        "            gr.Markdown(\"Input your goals and conditions to get a customized IELTS study plan.\")\n",
        "\n",
        "            with gr.Row():\n",
        "                target_score = gr.Slider(\n",
        "                    label=\"Target Overall Score\",\n",
        "                    minimum=5.0,\n",
        "                    maximum=9.0,\n",
        "                    step=0.5,\n",
        "                    value=7.0\n",
        "                )\n",
        "                weeks = gr.Slider(\n",
        "                    label=\"Available Study Weeks\",\n",
        "                    minimum=1,\n",
        "                    maximum=24,\n",
        "                    step=1,\n",
        "                    value=8\n",
        "                )\n",
        "\n",
        "            strengths = gr.Textbox(\n",
        "                label=\"Your Strengths\",\n",
        "                placeholder=\"e.g.: Listening, Reading...\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            weaknesses = gr.Textbox(\n",
        "                label=\"Areas to Improve\",\n",
        "                placeholder=\"e.g.: Writing, Speaking...\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            plan_btn = gr.Button(\"Generate Study Plan\", variant=\"primary\")\n",
        "            plan_output = gr.Markdown(label=\"Personalized Study Plan\")\n",
        "\n",
        "            plan_btn.click(\n",
        "                fn=generate_study_plan,\n",
        "                inputs=[target_score, weeks, strengths, weaknesses],\n",
        "                outputs=plan_output\n",
        "            )\n",
        "\n",
        "        # System Architecture Tab\n",
        "        with gr.Tab(\"System Architecture\"):\n",
        "            with gr.Row():\n",
        "                gr.Markdown(\"# IELTS Learning Assistant System Architecture\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"\"\"\n",
        "        ## Overall Architecture\n",
        "        This system uses Retrieval-Augmented Generation (RAG) combined with Text-to-Speech (TTS) technology to provide IELTS learning assistance.\n",
        "\n",
        "        ## How This System Works\n",
        "\n",
        "        This IELTS Learning Assistant uses a powerful combination of **Retrieval-Augmented Generation (RAG)** and **Text-to-Speech** technology to provide accurate, contextually relevant responses to your IELTS questions.\n",
        "\n",
        "        The system follows these steps:\n",
        "        1. When you ask a question, it's converted to a vector representation\n",
        "        2. This vector is compared against our IELTS materials database\n",
        "        3. The most relevant content is retrieved\n",
        "        4. The AI combines this content with its own knowledge to generate a helpful answer\n",
        "        5. If relevance is low, the AI relies more on its own knowledge\n",
        "\n",
        "        All answers are based on standard IELTS curriculum materials and best practices in IELTS preparation.\n",
        "        \"\"\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"\"\"\n",
        "        ## Components\n",
        "        1. **Data Preprocessing Module**\n",
        "          - Splits IELTS textbook materials into semantic chunks\n",
        "          - Generates embedding vectors using SentenceTransformer\n",
        "          - Stores text chunks and corresponding embedding vectors\n",
        "\n",
        "        2. **Retrieval Module**\n",
        "          - Uses vector similarity search\n",
        "          - Calculates similarity based on all-mpnet-base-v2 model\n",
        "          - Selects the N most relevant text chunks\n",
        "\n",
        "        3. **Generation Module**\n",
        "          - Uses Google Gemma-2b-it model\n",
        "          - Constructs context prompts based on retrieved content\n",
        "          - Generates answers based on relevance threshold\n",
        "\n",
        "        4. **Text-to-Speech Module**\n",
        "          - Uses Facebook MMS-TTS-Eng model\n",
        "          - Converts generated text to natural speech\n",
        "        \"\"\")\n",
        "\n",
        "            # Display the enhanced system diagram\n",
        "            gr.Image(value=generate_system_diagram(), label=\"System Flow Diagram\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"\"\"\n",
        "        ## Learning & Assessment Components\n",
        "\n",
        "        5. **Writing Assessment Module**\n",
        "          - Evaluates IELTS writing using large language model\n",
        "          - Provides detailed scoring and improvement suggestions\n",
        "\n",
        "        6. **Study Plan Module**\n",
        "          - Generates personalized learning plans based on user goals and time\n",
        "          - Provides targeted learning suggestions and resource recommendations\n",
        "        \"\"\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"\"\"\n",
        "        7. **Practice Test Module**\n",
        "          - Generates IELTS practice questions for each section\n",
        "          - Helps users familiarize with test format and content\n",
        "\n",
        "        8. **Learning Progress Tracking**\n",
        "          - Records user learning activities and achievements\n",
        "          - Provides visualization of learning journey\n",
        "        \"\"\")\n",
        "\n",
        "            with gr.Accordion(\"Technical Details\", open=False):\n",
        "                gr.Markdown(\"\"\"\n",
        "        ### Implementation Details\n",
        "\n",
        "        **Models Used:**\n",
        "        - **Embedding Model**: SentenceTransformer (all-mpnet-base-v2)\n",
        "        - **Language Model**: Google Gemma-2b-it (2 billion parameters)\n",
        "        - **TTS Model**: Facebook MMS-TTS-Eng\n",
        "\n",
        "        **RAG Implementation:**\n",
        "        ```python\n",
        "        # Vector search with relevance threshold\n",
        "        def retrieve_relevant_resources(query, embeddings, n_resources=5):\n",
        "            query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "            dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "            scores, indices = torch.topk(dot_scores, k=n_resources)\n",
        "            return scores, indices\n",
        "\n",
        "        # Dynamic prompt selection based on relevance\n",
        "        def generate_answer(query, context_items, avg_relevance_score=0.0):\n",
        "            # Use threshold to decide prompt strategy\n",
        "            relevance_threshold = 0.65\n",
        "\n",
        "            if avg_relevance_score < relevance_threshold:\n",
        "                # Low relevance - rely on model knowledge\n",
        "                prompt = \"Here is a question about the IELTS exam. Since no sufficiently relevant reference materials were found, please use your own knowledge to answer.\\\\n\\\\nQuestion: \" + query + \"\\\\n\\\\nAnswer:\"\n",
        "            else:\n",
        "                # High relevance - use retrieved content\n",
        "                context = \"\\\\n\\\\n\".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "                prompt = \"Based on the following IELTS materials, answer the question:\\\\n\\\\nContent:\\\\n\" + context + \"\\\\n\\\\nQuestion: \" + query + \"\\\\n\\\\nAnswer:\"\n",
        "                \"\"\")\n",
        "    return demo\n",
        "\n",
        "# 5. Startup interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(share=True)"
      ],
      "metadata": {
        "id": "Z91VpUNlcZNU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}